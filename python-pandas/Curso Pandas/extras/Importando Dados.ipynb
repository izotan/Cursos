{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c7f7fb",
   "metadata": {},
   "source": [
    "## Instalando bibliotecas necessárias para o ambiente python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485d1f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dionatan\\anaconda3\\envs\\alura_pandas\\lib\\site-packages (from requests) (2021.5.30)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-2.0.12 idna-3.4 requests-2.27.1 urllib3-1.26.16\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=8fc6ca67d281717aa605c882c1ca49d30fc4f53b48884c66c1168a4360bde499\n",
      "  Stored in directory: c:\\users\\dionatan\\appdata\\local\\pip\\cache\\wheels\\19\\f5\\6d\\a97dd4f22376d4472d5f4c76c7646876052ff3166b3cf71050\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.3.2.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7284ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para Web Scraping utilizado para evitar o retorno de um HTTP Error 403.\n",
    "# Erro ocorre ao tentar ler as tabelas com pd.read_html.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://www.federalreserve.gov/releases/h3/current/default.htm'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "table = soup.findAll('table')\n",
    "html_file = f'<html><body>{table}</body></html>'\n",
    "df = pd.read_html(html_file)\n",
    "\n",
    "# Como a função read_html retorna uma lista de DataFrames, basta acessar as tabelas pelos índices da lista.\n",
    "# Como temos três tabelas na página usamos os índices 0, 1 ou 2 para acessar os DataFrames que buscamos\n",
    "df[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
